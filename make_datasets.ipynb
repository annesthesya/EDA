{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69d1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install pandas\n",
    "# !pip install emoji\n",
    "# !pip install num2words\n",
    "# !pip install nltk\n",
    "# !pip install matplotlib\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ad6cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T19:23:44.963368800Z",
     "start_time": "2024-01-16T19:23:32.246531400Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import emoji\n",
    "#from num2words import num2words\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem import PorterStemmer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "#from matplotlib import colors\n",
    "#from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f4db0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T19:23:04.749807600Z",
     "start_time": "2024-01-16T19:23:04.732843200Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_df_from_json(url):\n",
    "    data_list = []\n",
    "\n",
    "    with open(url, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            label = \"\"\n",
    "            if \"overall\" in data and \"reviewText\" in data and \"summary\" in data:\n",
    "                if data[\"overall\"] == 3.0:\n",
    "                    label = \"neutral\"\n",
    "                elif data[\"overall\"] > 3.0:\n",
    "                    label = \"positive\"\n",
    "                elif data[\"overall\"] < 3.0:\n",
    "                    label = \"negative\"\n",
    "                data_list.append({\"label\": label, \"overall\": data[\"overall\"], \"reviewText\": data[\"reviewText\"], \"summary\": data[\"summary\"]})\n",
    "\n",
    "        df = pd.DataFrame(data_list)\n",
    "        df.drop_duplicates(subset=[\"reviewText\"], inplace=True)\n",
    "    \n",
    "        return df\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b560ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T19:23:22.095139600Z",
     "start_time": "2024-01-16T19:23:22.076507400Z"
    }
   },
   "outputs": [],
   "source": [
    "def concatenate_dfs(list_of_links):\n",
    "    dfs = []\n",
    "\n",
    "    for link in list_of_links:\n",
    "        dfs.append(read_df_from_json(link))\n",
    "    \n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return big_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44514e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T19:28:54.546145800Z",
     "start_time": "2024-01-16T19:23:51.209531800Z"
    }
   },
   "outputs": [],
   "source": [
    "links = ['data\\AMAZON_FASHION_5.json','data\\All_Beauty_5.json','data\\Luxury_Beauty_5.json','data\\Clothing_Shoes_and_Jewelry_5.json']\n",
    "df  = concatenate_dfs(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce891110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T19:30:22.475934Z",
     "start_time": "2024-01-16T19:30:19.048156200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutre: 876977\n",
      "pozitive: 6520468\n",
      "negative: 901322\n"
     ]
    }
   ],
   "source": [
    "print(\"neutre:\", df[\"label\"].value_counts()[\"neutral\"])\n",
    "print(\"pozitive:\", df[\"label\"].value_counts()[\"positive\"])\n",
    "print(\"negative:\", df[\"label\"].value_counts()[\"negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd93b4b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T19:48:53.427122800Z",
     "start_time": "2024-01-16T19:48:53.355540700Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_and_write_csv(input_df, train_size_per_class={'negative':17000, 'neutral':11000, 'positive':17000 }, test_size_per_class={'negative':2250, 'neutral':1500, 'positive':2250 }, validation_size_per_class={'negative':5000, 'neutral':3000, 'positive':5000 }):\n",
    "    grouped = input_df.groupby('label')\n",
    "\n",
    "    train_data = pd.DataFrame()\n",
    "    train_labels = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "    test_labels = pd.DataFrame()\n",
    "    validation_data = pd.DataFrame()\n",
    "    validation_labels = pd.DataFrame()\n",
    "    \n",
    "    train_data_plot = pd.DataFrame()\n",
    "    test_data_plot = pd.DataFrame()\n",
    "    validation_data_plot = pd.DataFrame()\n",
    "\n",
    "    for group_name, group_df in grouped:\n",
    "        # print(test_size_per_class[group_name])\n",
    "        shuffled_group_df = group_df.sample(frac=1, random_state=42)\n",
    "\n",
    "        train_group, test_group = train_test_split(shuffled_group_df, train_size=train_size_per_class[group_name] , test_size=test_size_per_class[group_name] + validation_size_per_class[group_name], random_state=42)\n",
    "    \n",
    "        test_group, validation_group = train_test_split(test_group, test_size=validation_size_per_class[group_name], random_state=42)\n",
    "\n",
    "        train_data = pd.concat([train_data, train_group[['label','reviewText']]])\n",
    "        train_labels = pd.concat([train_labels, train_group['label']])\n",
    "\n",
    "        test_data = pd.concat([test_data, test_group['reviewText']])\n",
    "        test_labels = pd.concat([test_labels, test_group['label']])\n",
    "    \n",
    "        validation_data = pd.concat([validation_data, validation_group['reviewText']])\n",
    "        validation_labels = pd.concat([validation_labels, validation_group['label']])\n",
    "    \n",
    "        train_data_plot = pd.concat([train_data_plot, train_group])  \n",
    "        test_data_plot = pd.concat([test_data_plot, test_group])\n",
    "        validation_data_plot = pd.concat([validation_data_plot, validation_group])\n",
    "        \n",
    "    \n",
    "    shuffled_train_data = train_data.sample(frac=1, random_state=42)\n",
    "    shuffled_train_labels = train_labels.sample(frac=1, random_state=42)\n",
    "\n",
    "    shuffled_train_data.rename(columns={0: \"text\"}, inplace=True)\n",
    "    shuffled_train_labels.rename(columns={0: \"label\"}, inplace=True)\n",
    "    \n",
    "    # shuffled_val_data = validation_data.sample(frac=1, random_state=42)\n",
    "    # shuffled_val_labels = validation_data.sample(frac=1, random_state=42)\n",
    "\n",
    "    validation_data.rename(columns={0: \"text\"}, inplace=True)\n",
    "    validation_labels.rename(columns={0: \"label\"}, inplace=True)\n",
    "    \n",
    "    test_data.rename(columns={0: \"text\"}, inplace=True)\n",
    "    test_labels.rename(columns={0: \"label\"}, inplace=True)\n",
    "    \n",
    "    shuffled_train_data.to_csv('data/shuffled_train_data.csv', index=False)\n",
    "    shuffled_train_labels.to_csv('data/shuffled_train_labels.csv', index=False)\n",
    "    validation_data.to_csv('data/validation_data.csv', index=False)\n",
    "    validation_labels.to_csv('data/validation_labels.csv', index=False)\n",
    "    test_data.to_csv('data/test_data.csv', index=False)\n",
    "    test_labels.to_csv('data/test_labels.csv', index=False)\n",
    "    train_data_plot.to_csv('data/train_data_plot.csv', index=False)\n",
    "    test_data_plot.to_csv('data/test_data_plot.csv', index=False)\n",
    "    validation_data_plot.to_csv('data/validation_data_plot.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "525ed7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T19:49:11.723983600Z",
     "start_time": "2024-01-16T19:48:54.616161600Z"
    }
   },
   "outputs": [],
   "source": [
    "split_and_write_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c6bda89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T19:39:51.716613100Z",
     "start_time": "2024-01-16T19:39:48.889806400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(45000, 1)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/shuffled_train_data.csv',index_col=None)\n",
    "train_labels = pd.read_csv('data/shuffled_train_labels.csv',index_col=None)\n",
    "\n",
    "test_data = pd.read_csv('data/test_data.csv',index_col=None)\n",
    "test_labels = pd.read_csv('data/test_labels.csv',index_col=None)\n",
    "\n",
    "val_data = pd.read_csv('data/validation_data.csv',index_col=None)\n",
    "val_labels = pd.read_csv('data/validation_labels.csv',index_col=None)\n",
    "\n",
    "train_data_plot = pd.read_csv('data/train_data_plot.csv',index_col=None)\n",
    "test_data_plot = pd.read_csv('data/test_data_plot.csv',index_col=None)\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a735c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
