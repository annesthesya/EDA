{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239ed2d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:34.290087100Z",
     "start_time": "2023-12-30T09:45:32.665325800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tiast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tiast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import emoji\n",
    "from num2words import num2words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from matplotlib import colors\n",
    "# from wordcloud import WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ad85d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:34.308850400Z",
     "start_time": "2023-12-30T09:45:34.288087500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tiast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tiast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install ipynb \n",
    "from ipynb.fs.full.preprocessor_class import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4478d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:35.019344600Z",
     "start_time": "2023-12-30T09:45:34.308850400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(45000, 1)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/shuffled_train_data.csv',index_col=None)\n",
    "train_labels = pd.read_csv('data/shuffled_train_labels.csv',index_col=None)\n",
    "\n",
    "test_data = pd.read_csv('data/test_data.csv',index_col=None)\n",
    "test_labels = pd.read_csv('data/test_labels.csv',index_col=None)\n",
    "\n",
    "train_data_plot = pd.read_csv('data/train_data_plot.csv',index_col=None)\n",
    "test_data_plot = pd.read_csv('data/test_data_plot.csv',index_col=None)\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdfb215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:41.268293800Z",
     "start_time": "2023-12-30T09:45:35.019344600Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(set(stopwords.words('english')), WordNetLemmatizer(), PorterStemmer(), True, True, True, True, True, False, True, True, True, False)\n",
    "\n",
    "preprocessed_data_train = pd.DataFrame(columns=['text'])\n",
    "preprocessed_data_test = pd.DataFrame(columns=['text'])\n",
    "\n",
    "preprocessed_data_train['text'] = train_data.apply(lambda row: preprocessor.preprocess(row.iloc[0]), axis = 1)\n",
    "\n",
    "preprocessed_data_test['text'] = test_data.apply(lambda row: preprocessor.preprocess(row.iloc[0]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c8ef13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:41.286188100Z",
     "start_time": "2023-12-30T09:45:41.274815800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0                               [love, super, comfortable]\n1        [shave, cremes, need, lather, feeling, fullnes...\n2        [love, bright, color, dress, fabric, nice, fee...\n3           [see, small, got, curve, get, fit, like, size]\n4        [sister, love, wolf, go, wrong, cute, comfy, w...\n                               ...                        \n44995    [sandal, poorly, constructed, base, shoe, fitt...\n44996    [wow, shirt, comfortable, fit, hug, body, soft...\n44997    [problem, heel, pain, pf, looking, good, walki...\n44998    [keep, spotting, hand, everything, blue, ink, ...\n44999            [light, died, right, away, poor, quality]\nName: text, Length: 45000, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a95b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:41.396793300Z",
     "start_time": "2023-12-30T09:45:41.282185700Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_data_test['text'] = preprocessed_data_test['text'].apply(lambda x: ' '.join(map(str, x)))\n",
    "preprocessed_data_train['text'] = preprocessed_data_train['text'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f2eabf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:46:05.809524700Z",
     "start_time": "2023-12-30T09:46:03.088327100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: C:\\Users\\tiast\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install tensorflow\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m \n\u001B[0;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mSequential([\n\u001B[0;32m      5\u001B[0m   tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m512\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;66;03m# we add a fully connected layer with 512 units using relu as activation function\u001B[39;00m\n\u001B[0;32m      6\u001B[0m   tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m512\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;66;03m# we add a fully connected layer with 512 units using relu as activation function\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m   tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m10\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# we add a fully connected layer with 10 units (the number of classes) using softmax as activation function\u001B[39;00m\n\u001B[0;32m     11\u001B[0m ])\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "import tensorflow as tf \n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(512, activation='relu'), # we add a fully connected layer with 512 units using relu as activation function\n",
    "  tf.keras.layers.Dense(512, activation='relu'), # we add a fully connected layer with 512 units using relu as activation function\n",
    "  tf.keras.layers.Dense(1024, activation='relu'), # we add a fully connected layer with 1024 units using relu as activation function\n",
    "  tf.keras.layers.Dense(1024, activation='relu'), # we add a fully connected layer with 1024 units using relu as activation function\n",
    "  tf.keras.layers.Dropout(0.4), # we add a dropout layer with the dropout rate 0.4\n",
    "  tf.keras.layers.Dense(10, activation='softmax') # we add a fully connected layer with 10 units (the number of classes) using softmax as activation function\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fcd14",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.082777Z"
    }
   },
   "outputs": [],
   "source": [
    "# import SGD and Adam optimizers \n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop \n",
    "\n",
    "# define the optimizer, set the learning rate and the momentum (for SGD with momentum)\n",
    "optimizer = SGD(learning_rate=0.001, momentum=0.9)  \n",
    "# compile the model specifying the optimizer, the loss and the metrics (as an array).\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94bcb0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.086775200Z"
    }
   },
   "outputs": [],
   "source": [
    "#from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "# The TextVectorization will build a vocabulary with at most 50_000 words\n",
    "# And when passing a sentence Tensor, will ouptut a tensor of seq_len size\n",
    "#vectorizer = TextVectorization(max_tokens=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1adcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:42.099297600Z",
     "start_time": "2023-12-30T09:45:42.089775500Z"
    }
   },
   "outputs": [],
   "source": [
    "#X_train = vectorizer(np.array([[word] for word in preprocessed_data_train['text']])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7442d16",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.094300Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(preprocessed_data_train['text']) \n",
    "test_tfidf = tfidf_vectorizer.transform(preprocessed_data_test['text'])\n",
    "# knn_model = KNeighborsClassifier(n_neighbors = neighbors, weights = weights, metric = metric)\n",
    "# knn_model.fit(train_tfidf, train_labels['label'])\n",
    "# predictions = knn_model.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c84b73",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.096293200Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4e1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:42.101825800Z",
     "start_time": "2023-12-30T09:45:42.100812200Z"
    }
   },
   "outputs": [],
   "source": [
    "list = []\n",
    "for el in train_labels['label']:\n",
    "    if el == 'positive':\n",
    "        list.append(1)\n",
    "    if el == 'negative':\n",
    "        list.append(2)\n",
    "    if el == 'neutral':\n",
    "        list.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f2452",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.104836600Z"
    }
   },
   "outputs": [],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e081aae",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.108835500Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98eb1e5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.112358500Z"
    }
   },
   "outputs": [],
   "source": [
    "for el in train_tfidf[:10]:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be8ddf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.115368900Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tfidf = np.asarray(train_tfidf).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8124f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.118369900Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(train_tfidf, np.array(list),\n",
    "          epochs=10, batch_size=32, initial_epoch=0,\n",
    "          validation_data=(train_tfidf, np.array(list))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206a8c8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.120878100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b971c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:42.146001700Z",
     "start_time": "2023-12-30T09:45:42.123890500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa06771",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.125901900Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "!pip install tensorflow-hub\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871023b7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.127901300Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3377971",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.130905100Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152c827",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.133207100Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f319946",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.135209400Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c06f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.137209900Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"tensorflow==2.6.*\"\n",
    "!pip install \"tensorflow-text==2.6.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5524ac4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.139474700Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89837f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.142001700Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c32caf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.146001700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bert layers\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Neural network layers\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "# Use inputs and outputs to construct a final model\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad4b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T09:45:42.235610100Z",
     "start_time": "2023-12-30T09:45:42.149004500Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 2), max_features = 5000)\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(preprocessed_data_train['text']) \n",
    "test_tfidf = tfidf_vectorizer.transform(preprocessed_data_test['text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae6455",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.150003800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b527b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.152524600Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')\n",
    "# Bert layers\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "# Neural network layers\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "# Use inputs and outputs to construct a final model\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_tfidf, train_labels['label'], epochs=2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684529cd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-30T09:45:42.154538400Z"
    }
   },
   "outputs": [],
   "source": [
    "y_predicted = model.predict(test_tfidf)\n",
    "y_predicted = y_predicted.flatten()\n",
    "print(y_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
